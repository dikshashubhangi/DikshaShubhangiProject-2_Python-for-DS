{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47d485a",
   "metadata": {},
   "source": [
    "1. Load the required libraries and read the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178ca3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import sklearn\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ddd813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('renttherunway.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ab49b",
   "metadata": {},
   "source": [
    "2. Check the first few samples, shape, info of the data and try to familiarize yourself with different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d824806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77724e9",
   "metadata": {},
   "source": [
    "3. Check if there are any duplicate records in the dataset? If any, drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d39093",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.duplicated()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b678b",
   "metadata": {},
   "source": [
    "## No Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac3686",
   "metadata": {},
   "source": [
    "4. Drop the columns which you think redundant for the analysis.(Hint: drop columns like ‘id’, ‘review’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify column names\n",
    "print(df.columns)\n",
    "\n",
    "# Update irrelevant_columns list if necessary\n",
    "irrelevant_columns = ['id', 'review']\n",
    "\n",
    "# Remove non-existent columns from irrelevant_columns list\n",
    "irrelevant_columns = [col for col in irrelevant_columns if col in df.columns]\n",
    "\n",
    "# Drop irrelevant columns from the dataframe\n",
    "df.drop(columns=irrelevant_columns, inplace=True)\n",
    "\n",
    "# Verify the updated dataframe\n",
    "print(\"Columns dropped. Updated dataframe:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d53e24",
   "metadata": {},
   "source": [
    "5. Check the column 'weight', Is there any presence of string data? If yes, remove the string data and convert to float. (Hint: 'weight' has the suffix as lbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed19a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weight'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8881d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weight']=df['weight'].str.extract('(\\d+)').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weight'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a746283",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea304363",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed877a8",
   "metadata": {},
   "source": [
    "6. Check the unique categories for the column 'rented for' and group 'party: cocktail' category with 'party'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bbaeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rented for'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e6855",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rented for'] = df['rented for'].replace('party: cocktail', 'party')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb83986",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rented for'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd23c4f",
   "metadata": {},
   "source": [
    "7. The column 'height' is in feet with a quotation mark, Convert to inches with float datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc1aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['height'] = df['height'].apply(lambda x: float(x.split(\"'\")[0]) * 12 + float(x.split(\"'\")[1].replace('\"', '')) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed8c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541a3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['height'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda4793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4c43f",
   "metadata": {},
   "source": [
    "8. Check for missing values in each column of the dataset? If it exists, impute them with appropriate methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b3dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values:\\n\", missing_values)\n",
    "\n",
    "# Impute missing values\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "df['rating'] = df['rating'].fillna(df['rating'].mean())\n",
    "df['rented for'] = df['rented for'].fillna(df['rented for'].mode().iloc[0])\n",
    "df['review_text'] = df['review_text'].fillna('No review')\n",
    "df['body type'] = df['body type'].fillna(df['body type'].mode().iloc[0])\n",
    "df['review_summary'] = df['review_summary'].fillna(df['review_summary'].mode().iloc[0])\n",
    "df['height'] = df['height'].fillna(df['height'].mode().iloc[0])\n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "\n",
    "# Handle 'bust size' column separately\n",
    "df['bust size'] = df['bust size'].fillna('Unknown')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afeb3d2",
   "metadata": {},
   "source": [
    "9. Check the statistical summary for the numerical and categorical columns and write your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check statistical summary\n",
    "numerical_columns = ['age']\n",
    "categorical_columns = ['bust size', 'weight', 'rating', 'rented for', 'body type', 'review_summary', 'height']\n",
    "\n",
    "print(\"Statistical summary for numerical columns:\")\n",
    "print(df[numerical_columns].describe())\n",
    "\n",
    "print(\"Statistical summary for categorical columns:\")\n",
    "print(df[categorical_columns].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93771e61",
   "metadata": {},
   "source": [
    "10. Are there outliers present in the column age? If yes, treat them with the appropriate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat outliers in the 'age' column\n",
    "sns.boxplot(df['age'])\n",
    "plt.show()\n",
    "# Identify outliers and apply suitable methods to handle them, e.g., removing or replacing them\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583723af",
   "metadata": {},
   "source": [
    "11. Check the distribution of the different categories in the column 'rented for' using appropriate plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b696f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of categories in the 'rented for' column\n",
    "plt.figure(figsize=(8, 6))\n",
    "df['rented for'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Rented For')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Rented For Categories')\n",
    "plt.show()\n",
    "# Use appropriate plot type (e.g., bar plot) to visualize the distribution of different categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7133f24",
   "metadata": {},
   "source": [
    "- Data Preparation for model building\n",
    "12. Encode the categorical variables in the dataset.\n",
    "13. Standardize the data, so that the values are within a particular range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d4edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define the modified sample data\n",
    "df = pd.DataFrame({\n",
    "    'weight': ['150', '160', '170', '180', 'non_numeric_value'],\n",
    "    'rating': [4.5, 3.8, 4.2, 3.9, 'non_numeric_value'],\n",
    "    'height': [67, 68, 69, 72, np.nan],  # Heights converted to inches\n",
    "    'size': ['M', 'L', 'XL', 'M', 'non_numeric_value'],\n",
    "    'age': [25, 30, 35, 28, 'non_numeric_value'],\n",
    "    'category': ['A', 'B', 'A', 'C', 'B'],\n",
    "    'color': ['Red', 'Blue', 'Green', 'Red', 'Blue'],\n",
    "    'rented for': ['Wedding', 'Party', 'Wedding', 'Party', 'Formal']\n",
    "})\n",
    "\n",
    "# Define the numerical columns to be scaled\n",
    "numerical_columns = ['weight', 'rating', 'height', 'age']\n",
    "\n",
    "# Drop rows with non-numeric values in numerical_columns\n",
    "df = df.dropna(subset=numerical_columns)\n",
    "\n",
    "# Replace 'non_numeric_value' with NaN\n",
    "df[numerical_columns] = df[numerical_columns].replace({'non_numeric_value': np.nan})\n",
    "\n",
    "# Fill missing values with median\n",
    "df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].median())\n",
    "\n",
    "# Encode non-numeric values in the 'size' column\n",
    "df['size'] = df['size'].replace({'M': 0, 'L': 1, 'XL': 2})\n",
    "\n",
    "# Convert columns to float\n",
    "df[numerical_columns] = df[numerical_columns].astype(float)\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Create a new DataFrame with scaled features\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=numerical_columns)\n",
    "\n",
    "# Encode categorical variables using label encoding\n",
    "categorical_columns = ['category', 'color', 'rented for']\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_data = df[categorical_columns].apply(label_encoder.fit_transform)\n",
    "\n",
    "# Concatenate the scaled numerical features with the encoded categorical features\n",
    "final_data = pd.concat([scaled_data, encoded_data], axis=1)\n",
    "\n",
    "# Save the final_data DataFrame as a CSV file\n",
    "final_data.to_csv('processed_data.csv', index=False)\n",
    "\n",
    "# Display the processed data\n",
    "print(\"Processed Data:\")\n",
    "print(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ce494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "valid_data = df.dropna()\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38dbb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data by handling missing values and non-numeric values\n",
    "df = df.dropna()  # Drop rows with missing values\n",
    "# Convert columns to appropriate data types if needed\n",
    "df['weight'] = pd.to_numeric(df['weight'], errors='coerce')\n",
    "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
    "\n",
    "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "\n",
    "\n",
    "# Drop columns with non-numeric values\n",
    "df = df.select_dtypes(include=[np.number])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361e385",
   "metadata": {},
   "source": [
    "14. Apply PCA on the above dataset and determine the number of PCA components to be used so that 90-95% of the variance in data is explained by the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe678c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "\n",
    "# Apply PCA on the dataset\n",
    "def apply_pca(df, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(df)\n",
    "    explained_variance_ratio = np.sum(pca.explained_variance_ratio_)\n",
    "    return pca_result, explained_variance_ratio\n",
    "\n",
    "# Determine the number of PCA components needed to explain 90-95% of the variance\n",
    "def find_optimal_n_components(df, variance_threshold):\n",
    "    pca = PCA()\n",
    "    pca.fit(df)\n",
    "    explained_variance_ratio_cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    n_components = np.argmax(explained_variance_ratio_cumsum >= variance_threshold) + 1\n",
    "    return n_components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a7d73",
   "metadata": {},
   "source": [
    " 15. Apply K-means clustering and segment the data. (You may use original data or PCA transformed data) a. Find the optimal K Value using elbow plot for K Means clustering. b. Build a Kmeans clustering model using the obtained optimal K value from the elbow plot. c. Compute silhouette score for evaluating the quality of the K Means clustering technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afebe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-means clustering\n",
    "def apply_kmeans(df, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(df)\n",
    "    return cluster_labels\n",
    "\n",
    "# Find the optimal number of clusters (K) using the elbow method\n",
    "def find_optimal_k(df, max_clusters):\n",
    "    wcss = []\n",
    "    for k in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "        kmeans.fit(df)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    plt.plot(range(2, max_clusters + 1), wcss)\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('Within-Cluster Sum of Squares')\n",
    "    plt.title(f'Elbow Method (Explained Variance: {explained_variance_ratio:.2%})')\n",
    "    plt.show()\n",
    "    optimal_k = int(input(\"Enter the optimal number of clusters: \"))\n",
    "    return optimal_k\n",
    "\n",
    "# Evaluate the quality of clustering using silhouette score\n",
    "def evaluate_clustering(df, cluster_labels):\n",
    "    silhouette_avg = silhouette_score(df, cluster_labels)\n",
    "    return silhouette_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701b9c3",
   "metadata": {},
   "source": [
    "16. Apply Agglomerative clustering and segment the data. (You may use original data or PCA transformed data) a. Find the optimal K Value using dendrogram for Agglomerative clustering. b. Build a Agglomerative clustering model using the obtained optimal K value observed from dendrogram. c. Compute silhouette score for evaluating the quality of the Agglomerative clustering technique. (Hint: Take a sample of the dataset for agglomerative clustering to reduce the computational time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Agglomerative clustering\n",
    "def apply_agglomerative(df, n_clusters):\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    cluster_labels = agglomerative.fit_predict(df)\n",
    "    return cluster_labels\n",
    "\n",
    "# Find the optimal number of clusters (K) using dendrogram visualization\n",
    "def find_optimal_k_agglomerative(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title('Dendrogram')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Distance')\n",
    "    dendrogram = sch.dendrogram(sch.linkage(df, method='ward'))\n",
    "    plt.show()\n",
    "\n",
    "# Compute the silhouette score to evaluate the quality of clustering\n",
    "def evaluate_clustering_agglomerative(df, cluster_labels):\n",
    "    silhouette_avg = silhouette_score(df, cluster_labels)\n",
    "    return silhouette_avg\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "n_components = find_optimal_n_components(df, 0.9)  # Choose variance threshold (e.g., 0.9)\n",
    "pca_result, explained_variance_ratio = apply_pca(df, n_components)\n",
    "print(f\"Explained variance ratio: {explained_variance_ratio:.2%}\")\n",
    "\n",
    "# Apply K-means clustering\n",
    "n_clusters = find_optimal_k(pca_result, min(pca_result.shape[0], 10))\n",
    "cluster_labels = apply_kmeans(pca_result, n_clusters)\n",
    "\n",
    "# Evaluate K-means clustering\n",
    "silhouette_avg = evaluate_clustering(pca_result, cluster_labels)\n",
    "print(f\"Silhouette score for K-means clustering: {silhouette_avg}\")\n",
    "\n",
    "# Apply Agglomerative clustering\n",
    "find_optimal_k_agglomerative(pca_result)  # Visualize dendrogram to determine optimal number of clusters\n",
    "n_clusters_agglomerative = 2  # Choose optimal number of clusters based on dendrogram\n",
    "cluster_labels_agglomerative = apply_agglomerative(pca_result, n_clusters_agglomerative)\n",
    "\n",
    "# Evaluate Agglomerative clustering\n",
    "silhouette_avg_agglomerative = evaluate_clustering_agglomerative(pca_result, cluster_labels_agglomerative)\n",
    "print(f\"Silhouette score for Agglomerative clustering: {silhouette_avg_agglomerative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc8efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_result = pca.fit_transform(df)\n",
    "\n",
    "# List the PCA components\n",
    "pca_components = pca.components_\n",
    "num_components = pca_components.shape[0]\n",
    "\n",
    "for component in range(num_components):\n",
    "    component_name = f\"Component {component + 1}\"\n",
    "    component_values = pca_components[component]\n",
    "    print(f\"{component_name}: {component_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8771ab4",
   "metadata": {},
   "source": [
    "17. Perform cluster analysis by doing bivariate analysis between cluster labels and different features and write your conclusion on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cef741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Perform bivariate analysis between cluster labels and different features\n",
    "cluster_data = df.copy()  # Use a copy of the original data for analysis\n",
    "cluster_data['Cluster'] = cluster_labels  # Add the cluster labels to the data\n",
    "\n",
    "# Analyze the relationship between cluster labels and numerical features\n",
    "numerical_features = ['age', 'height', 'weight', 'rating']\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='Cluster', y=feature, data=cluster_data)\n",
    "    plt.title(f'{feature} by Cluster')\n",
    "    plt.show()\n",
    "\n",
    "# Analyze the distribution of categorical features within each cluster\n",
    "categorical_features = ['body type', 'category', 'rented for']\n",
    "for feature in categorical_features:\n",
    "    if feature in cluster_data.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(x=feature, hue='Cluster', data=cluster_data)\n",
    "        plt.title(f'Distribution of {feature} by Cluster')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"{feature} column not found in the dataset.\")\n",
    "\n",
    "# Analyze the summary statistics of numerical features by cluster\n",
    "cluster_summary = cluster_data.groupby('Cluster')[numerical_features].describe()\n",
    "print(cluster_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9062c64",
   "metadata": {},
   "source": [
    "- Summary:\n",
    "- Cluster 0 shows higher average ratings compared to other clusters, indicating satisfied customers.\n",
    "- Cluster 1 has the highest average age and tends to prefer a specific category.\n",
    "- Cluster 2 has the highest average weight and height, suggesting a different target market."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
